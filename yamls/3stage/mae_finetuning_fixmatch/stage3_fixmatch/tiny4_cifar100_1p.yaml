vars:
  label_percentage: 0.01
  weight_decay: 0.05
  beta2: 0.999
  warmup_epochs: 5
  crop: 0.8
  randaug_num_ops: 1
  randaug_magnitude: 9
  randaug_magnitude_std: 0.5
  randaug_apply_op_p: 1
  color_jitter: 0.4
  label_smoothing: 0.1
  mixup: 0.8
  cutmix: 1.0
  supervised_mixup_mode: null
  unsupervised_mixup_mode: null
  drop_path: 0
  unlabeled_to_labeled_ratio: 5
  threshold: 0.6
  unsupervised_loss_weight: 5
  teacher_pseudo_labeling: true
  strong_augmentation_for_labeled: true
  continue_from_stage_name: cifar100_stage2_mae_finetuning
  continue_from_stage_id: 99e8ujyo
  continue_from_stage_checkpoint: last
  n_views: 2  # dependent on transforms of multi_view_wrapper
  batch_size: 50
  max_epochs: 50
  max_epochs_schedule: 250
  lr: 0.005
  target_factor: 0.9999
  layerwise_lr_decay: 0.75
  hpo_type: null
  prev_stage_fullname: CIFAR-100 MAE + Fine-Tuning 1% (MAE hpo 1op crop0.2 nodroppath nomixup lr0.002 lwlrd0.75 150ep BS48)
  
  name: ${vars.prev_stage_fullname} + FixMatch ${eval:int(${vars.label_percentage} * 100)}% (SemiViT ${eval:'' if '${vars.hpo_type}' == 'None' else '${vars.hpo_type} '}ckpt_${vars.continue_from_stage_checkpoint} ${vars.randaug_num_ops}op crop${vars.crop} ${eval:'no' if ${vars.drop_path} == 0.0 else ''}droppath ${eval:'no' if "${vars.supervised_mixup_mode}" == "None" else ''}mixup lr${vars.lr} lwlrd${vars.layerwise_lr_decay} tf${vars.target_factor} ${vars.max_epochs}${eval:'/${vars.max_epochs_schedule}' if ${vars.max_epochs} != ${vars.max_epochs_schedule} else ''}ep BS${vars.batch_size})

name: ${vars.name}
stage_name: cifar100_stage3_mae_finetuning_fixmatch
ignore_stage_name: true
datasets:
  train:
    kind: torchvision_dataset_wrapper
    dataset_identifier: cifar100
    num_classes: 100
    torchvision_args:
      kind: CIFAR100
      train: true
      download: false
    dataset_wrappers:
      # - kind: classwise_subset_wrapper
      #   end_index: 128
      - kind: semisupervised_wrapper
        labeled_percentage: ${eval:${vars.label_percentage}*100}
      - kind: semisupervised_oversampling_wrapper
        include_labeled_in_unlabeled: true
        unlabeled_to_labeled_ratio: ${vars.unlabeled_to_labeled_ratio}
    sample_wrappers:
      - kind: multi_view_wrapper
        transforms:
          - - interpolation: bicubic
              kind: kd_random_resized_crop
              scale:
                - ${vars.crop}
                - 1.0
              size: 32
            - kind: kd_random_horizontal_flip
            - kind: kd_color_jitter
              brightness: ${vars.color_jitter}
              contrast: ${vars.color_jitter}
              saturation: ${vars.color_jitter}
            - kind: kd_cifar100_norm
          - - interpolation: bicubic
              kind: kd_random_resized_crop
              scale:
                - ${vars.crop}
                - 1.0
              size: 32
            - kind: kd_random_horizontal_flip
            - kind: kd_rand_augment
              num_ops: ${vars.randaug_num_ops}
              magnitude: ${vars.randaug_magnitude}
              magnitude_std: ${vars.randaug_magnitude_std}
              apply_op_p: ${vars.randaug_apply_op_p}
              interpolation: bicubic
              fill_color: [ 125, 123, 114 ]
            - kind: kd_cifar100_norm
    batch_wrappers:
      - kind: prob_pseudo_mix_batch_wrapper
        model_name: semivit
        prediction_head_name: fixmatch
        n_classes: 100
        weak_augmentation_index: 0
        label_smoothing: ${vars.label_smoothing}
        mixup_alpha: ${vars.mixup}
        cutmix_alpha: ${vars.cutmix}
        mixup_p: 0.5
        cutmix_p: 0.5
        shuffle_mode: flip
        supervised_mixup_mode: ${vars.supervised_mixup_mode}
        unsupervised_mixup_mode: ${vars.unsupervised_mixup_mode}
  train_unaugmented:
    kind: torchvision_dataset_wrapper
    dataset_identifier: cifar100
    num_classes: 100
    torchvision_args:
      kind: CIFAR100
      train: true
      download: false
    x_transform:
      - kind: kd_cifar100_norm
  test:
    kind: torchvision_dataset_wrapper
    dataset_identifier: cifar100
    num_classes: 100
    torchvision_args:
      kind: CIFAR100
      train: false
      download: false
    x_transform:
      - kind: kd_cifar100_norm
  test_small:
    kind: torchvision_dataset_wrapper
    dataset_identifier: cifar100
    num_classes: 100
    torchvision_args:
      kind: CIFAR100
      train: false
      download: false
    x_transform:
      - kind: kd_cifar100_norm
model:
  kind: mae_contheads_vit
  name: semivit
  target_factor: ${vars.target_factor}
  encoder:
    kind: vit.masked_encoder
    patch_size: 4
    embedding_dim: 192
    depth: 12
    attention_heads: 3
    optim:
      kind: adamw
      lr: ${vars.lr}
      lr_scaler:
        kind: linear_lr_scaler
        # scale lr with number of labeled samples only (-> increase divisor according to num of labeled samples)
        # lr = base_lr * batchsize_l/256
        # formula in code is base_lr * batchsize / divisor
        # to get the modified scaled LR:
        # - reverse multiplication by batchsize by dividing by batchsize
        # - calculate labeled batch size
        # - set divisor to inverse of desired lr scale factor -> 256/batchsize_l = 256*(u2lr+1)/batchsize
        divisor: ${eval:${vars.batch_size} * ${vars.n_views} * 256 / (${vars.batch_size} / (${vars.unlabeled_to_labeled_ratio}+1))}
      weight_decay: ${vars.weight_decay}
      betas:
        - 0.9
        - ${vars.beta2}
      param_group_modifiers:
        - decay: ${vars.layerwise_lr_decay}
          kind: layerwise_lr_decay_modifier
      schedule:
        - end_checkpoint:
            epoch: ${vars.warmup_epochs}
          exclude_first: true
          exclude_last: true
          kind: linear_increasing
        - exclude_last: true
          kind: cosine_decreasing
          end_checkpoint:
            epoch: ${vars.max_epochs_schedule}
    initializer:
      checkpoint: ${vars.continue_from_stage_checkpoint}
      kind: previous_run_initializer
      model_name: backbone_head.backbone
      stage_id: ${vars.continue_from_stage_id}
      stage_name: ${vars.continue_from_stage_name}
  contrastive_heads:
    fixmatch:
      kind: heads.fixmatch_head
      target_factor: ${vars.target_factor}
      nonaffine_batchnorm: true
      output_shape: 100
      threshold: ${vars.threshold}
      unsupervised_loss_weight: ${vars.unsupervised_loss_weight}
      teacher_pseudo_labeling: ${vars.teacher_pseudo_labeling}
      strong_augmentation_for_labeled: ${vars.strong_augmentation_for_labeled}
      pooling:
        kind: class_token
        # kind: mean_patch
      optim:
        kind: adamw
        lr: ${vars.lr}
        lr_scaler:
          kind: linear_lr_scaler
          # scale lr with number of labeled samples only (-> increase divisor according to num of labeled samples)
          # lr = base_lr * batchsize_l/256
          # formula in code is base_lr * batchsize / divisor
          # to get the modified scaled LR:
          # - reverse multiplication by batchsize by dividing by batchsize
          # - calculate labeled batch size
          # - set divisor to inverse of desired lr scale factor -> 256/batchsize_l = 256*(u2lr+1)/batchsize
          divisor: ${eval:${vars.batch_size} * ${vars.n_views} * 256 / (${vars.batch_size} / (${vars.unlabeled_to_labeled_ratio}+1))}
        betas:
          - 0.9
          - ${vars.beta2}
        weight_decay: ${vars.weight_decay}
        schedule:
          - end_checkpoint:
              epoch: ${vars.warmup_epochs}
            exclude_first: true
            exclude_last: true
            kind: linear_increasing
          - exclude_last: true
            kind: cosine_decreasing
            end_checkpoint:
              epoch: ${vars.max_epochs_schedule}
      initializer:
        checkpoint: ${vars.continue_from_stage_checkpoint}
        kind: previous_run_initializer
        model_name: backbone_head.head
        stage_id: ${vars.continue_from_stage_id}
        stage_name: ${vars.continue_from_stage_name}
trainer:
  kind: mae_contheads_vit_trainer
  max_epochs: ${vars.max_epochs}
  effective_batch_size: ${vars.batch_size}
  precision: bfloat16
  mask_generator:
    kind: random_mask_generator
    mask_ratio: 0.0
  normalize_pixels: true
  log_every_n_epochs: 1
  loggers:
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: supervised_loss
      invert_key: false
      category: loss
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: unsupervised_loss
      invert_key: false
      category: loss
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: unsupervised_loss_mean_over_threshold
      invert_key: false
      category: loss
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: samples_above_threshold
      invert_key: false
      category: confidence
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: classification_confidence_unlabeled
      invert_key: false
      category: confidence
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: classification_confidence_unlabeled_over_threshold
      invert_key: false
      category: confidence
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: classification_confidence_labeled
      invert_key: false
      category: confidence
    - every_n_epochs: 1
      kind: group_update_output_logger
      pattern: pseudo_label_accuracy
      invert_key: false
      category: confidence
    - kind: loss_logger
      every_n_epochs: 1
      dataset_key: train_unaugmented
    - kind: loss_logger
      every_n_epochs: 1
      dataset_key: test
    # for comparibility (add to all last steps):
    - kind: accuracy_logger
      every_n_epochs: 1
      dataset_key: train_unaugmented
      top_k:
        - 1
        - 5
      predict_kwargs:
        views:
          - 0
        dataset_key: train_unaugmented
    - kind: accuracy_logger
      every_n_epochs: 1
      dataset_key: test
      top_k:
        - 1
        - 5
      predict_kwargs:
        views:
          - 0
        dataset_key: test
    - kind: checkpoint_logger
      save_latest_optim: false
      save_optim: false
      every_n_epochs: ${eval:${vars.max_epochs}+1}
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy1/train_unaugmented*
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy1/train_unaugmented*
      log_absolute_best: true
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy1/test*
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy1/test*
      log_absolute_best: true
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy5/train_unaugmented*
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy5/train_unaugmented*
      log_absolute_best: true
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy5/test*
    - kind: best_metric_logger
      every_n_epochs: 1
      pattern: accuracy5/test*
      log_absolute_best: true
    - kind: feature_umap_logger
      dataset_key: test_small
      every_n_epochs: ${vars.max_epochs}
      n_components: 2
      n_neighbors: 100
      min_dist: 0.2
      metric: 'euclidean'
      extractors:
        - kind: generic_extractor
          model_property_path: contrastive_heads.fixmatch.target_head.pooling
    - kind: knn_metrics_logger
      every_n_epochs: ${vars.max_epochs}
      train_dataset_key: train_unaugmented
      test_dataset_key: test
      extractors:
        - kind: generic_extractor
          model_property_path: contrastive_heads.fixmatch.target_head.pooling
      knns:
        - 1
        - 2
        - 3
        - 5
        - 8
        - 13
        - 21
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn01/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn01/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn02/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn02/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn03/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn03/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn05/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn05/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn08/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn08/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn13/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn13/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn21/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn21/
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: knn_accuracy/knn*
    - kind: best_metric_logger
      every_n_epochs: ${vars.max_epochs}
      pattern: nn_purity/knn*


summary_summarizers:
  - kind: best_metric_summary_summarizer
    pattern: accuracy1/train_unaugmented*/last
  - kind: best_metric_summary_summarizer
    pattern: accuracy1/train_unaugmented*/max
  - kind: best_metric_summary_summarizer
    pattern: accuracy1/test*/last
  - kind: best_metric_summary_summarizer
    pattern: accuracy1/test*/max
  - kind: best_metric_summary_summarizer
    pattern: accuracy5/train_unaugmented*/last
  - kind: best_metric_summary_summarizer
    pattern: accuracy5/train_unaugmented*/max
  - kind: best_metric_summary_summarizer
    pattern: accuracy5/test*/last
  - kind: best_metric_summary_summarizer
    pattern: accuracy5/test*/max
  - kind: best_metric_summary_summarizer
    pattern: knn_accuracy/knn*/GenericExtractor-batchnorm/train_unaugmented-test/max
  - kind: best_metric_summary_summarizer
    pattern: nn_purity/knn*/GenericExtractor-batchnorm/train_unaugmented-test/max
