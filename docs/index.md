[[Code](https://github.com/ml-jku/MAE-CT)] [[Paper + Appendix](https://arxiv.org/abs/2304.10520)] [[Models](https://github.com/ml-jku/MAE-CT#pretrained-checkpoints)] [[BibTeX](https://github.com/ml-jku/MAE-CT#citation)] [[MIM-Refiner (Successor)](https://ml-jku.github.io/MIM-Refiner/)]

**M**asked **A**uto**E**ncoder: **C**ontrastive **T**uning tunes the representation of a pre-trained MAE to form semantic clusters via a NNCLR training stage.

<p align="center">
<img width="31.5%" alt="maect_schematic" src="https://raw.githubusercontent.com/ml-jku/MAE-CT/main/.github/schematic_contrastive_tuning.svg">
<img width="67%" alt="lowshot_vitl" src="https://raw.githubusercontent.com/ml-jku/MAE-CT/main/.github/lowshot_aug_L_white.svg">
</p>


